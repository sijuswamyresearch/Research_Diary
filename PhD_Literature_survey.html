<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Siju Swamy">
<meta name="dcterms.date" content="2025-02-25">

<title>Review of Literature in Core Research Area - February 25, 2025 – My Coursework Diary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">My Coursework Diary</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./daily_entry.html"> 
<span class="menu-text">Diary</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./PhD_Literature_survey.html" aria-current="page"> 
<span class="menu-text">Literature Review</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/sijuswamyresearch/sijuswamyresearch"> 
<span class="menu-text">GitHub</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Review of Literature in Core Research Area - February 25, 2025</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Siju Swamy </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="uncertainty-driven-loss-for-single-image-super-resolution" class="level1 page-columns page-full">
<h1>Uncertainty-Driven Loss for Single Image Super-Resolution</h1>
<section id="research-gap-identified-in-the-paper" class="level2">
<h2 class="anchored" data-anchor-id="research-gap-identified-in-the-paper">Research Gap Identified in the Paper</h2>
<p>The authors argued that traditional Mean Squared Error (MSE) or L1 loss functions used in DL based super resolution models treat every pixel equally, regardless of its importance to visual perception. This is problematic because texture and edge areas carry more vital visual information than smooth areas, and MSE/L1 don’t account for this spatial variation. The equal weightage is non-optimal because it does not adaptive to the local image features which is an open research problem. Existing deep learning-based Single Image Super-Resolution (SISR) methods primarily focus on increasing network depth and complexity, or introducing attention mechanisms, while still relying on MSE or L1 loss. The authors contend that these methods do not explicitly address how to prioritize pixels containing important visual information in a principled and adaptive manner during the training process.</p>
</section>
<section id="proposed-approach" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="proposed-approach">Proposed Approach</h2>
<p>In the context of SISR, authors coined a new term “uncertainty” refers to the inherent ambiguity in reconstructing a high-resolution (HR) image from a low-resolution (LR) counterpart. They used uncertainty as a measure of difficulty in accurate image reconstruction.</p>
<p>The authors propose an <em>adaptive weighted loss</em>, uncertainty driven loss (UDL), that prioritizes texture and edge pixels with high uncertainty during training. Unlike traditional methods, UDL assigns larger weights to these pixels, forcing the network to focus on accurately reconstructing them. This addresses the limitations of MSE/L1 by explicitly accounting for the spatial variation in importance across different image regions.</p>
<p>There are two classes of uncertainty in Bayesian modeling: aleatoric uncertainty capturing noise inherent in observation data and epistemic uncertainty accounting for uncertainty of model about its predictions. The authors formulated SISR as a Bayesian estimation problem using the aleatoric uncertainty where the goal is to estimate not only the Super Resolved (SR) image (mean) but also its uncertainty (variance) simultaneously. This approach allows them to model the aleatoric uncertainty inherent in the SR process and to leverage prior knowledge for regularization.</p>
<p>Let <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> denote the low resolution and the respective high resolution image respectively. If <span class="math inline">\(f(\cdot)\)</span> denotes an arbitrary SISR network and aleatoric uncertainty <span class="math inline">\(\theta_i\)</span>. The additive form of overall observation model can be written as:</p>
<p><span id="eq-observation_model"><span class="math display">\[
x_i = f(y_i) + \theta_i
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon\)</span> represents the Laplace distribution with zero mean and unit variance.</p>
<p>Traditional DL based models just focused on the mean, <span class="math inline">\(f(y_i)\)</span> and discard the variance term <span class="math inline">\(\theta_i\)</span>. For high level vision task it will not raise any issues. But this approach is not suitable for low-level vision tasks like SISR, where high-uncertainty pixels (e.g., texture and edge pixels) are visually more important and should be prioritized. This discrepancy motivates their approach of prioritizing pixels in low-level vision tasks.</p>
<p>For the <span class="math inline">\((x_i,y_i)\)</span> pair, the likelihood function is defined as:</p>
<p><span id="eq-eq2"><span class="math display">\[
p(x_i,\theta_i|y_i)=\dfrac{1}{2\theta_i}\text{exp}\left(-\dfrac{||x_i-f(y_i)||_1}{\theta_i}\right)
\tag{2}\]</span></span></p>
<p>Where <span class="math inline">\(f(y_i)\)</span> and <span class="math inline">\(\theta_i\)</span> denote the SR image and the uncertainty which are to be learned by a DL network respectively.</p>
<p>The log-likelihood function is written as:</p>
<p><span id="eq-loglikeli"><span class="math display">\[
\ln(p(x_i,\theta_i|y_i))=-\dfrac{||x_i-f(y_i)||_1}{\theta_i}-\ln(\theta_i)-\ln 2
\tag{3}\]</span></span></p>
<p>To address the numerical stability of the estimation <span class="math inline">\(s=-\ln (\theta_1)\)</span> will be estimated from the log-likelihood of <span class="math inline">\(N\)</span> samples defined by:</p>
<p><span id="eq-log"><span class="math display">\[
\mathcal{L}_{EU}=\frac{1}{N}\sum\limits_{i=1}^N\text{exp}(-s_i)||x_i-f(y_i)||_1+s_i
\tag{4}\]</span></span></p>
<p>So a MLE of <a href="#eq-loglikeli" class="quarto-xref">Equation&nbsp;3</a> is same as the minimization of <a href="#eq-log" class="quarto-xref">Equation&nbsp;4</a>. <span class="math inline">\(\mathcal{L}_{EU}\)</span> is called the estimating uncertainty loss for the SR problem.</p>
<p>The authors observe that most pixels in an image have relatively low uncertainty, while only a few texture and edge pixels have high uncertainty. By imposing a sparsity prior, they prevent the network from predicting high uncertainty for all pixels, leading to a more accurate and meaningful uncertainty estimation. The Jeffrey’s prior, <span class="math inline">\(p(\theta_i)\propto \dfrac{1}{\theta_i}\)</span> is used to encourage sparsity in the uncertainty map. Using the Bayer’s probability:</p>
<p><span id="eq-bayes"><span class="math display">\[
p(x_i,\theta_i|y_i)=p(x_i|y_i,\theta_i)\cdot p(\theta_i)=\dfrac{1}{2\theta_i}\text{exp}\left(-\dfrac{||x_i-f(y_i)||_1}{\theta_i}\right)\cdot \frac{1}{\theta_i}=\dfrac{1}{2\theta_i^2}\text{exp}\left(-\dfrac{||x_i-f(y_i)||_1}{\theta_i}\right)
\tag{5}\]</span></span></p>
<p>The maximum likelihood estimate for logarithm of <span class="math inline">\(\theta_i=s\)</span> is the minimization of log likelihood of the joint distribution of <a href="#eq-bayes" class="quarto-xref">Equation&nbsp;5</a> on <span class="math inline">\(N\)</span> samples.</p>
<p>Authors proposed this function as their new loss function as:</p>
<p><span id="eq-ESU"><span class="math display">\[
\mathcal{L}_{ESU}=\frac{1}{N}\sum\limits_{i=1}^N\text{exp}(-s_i)||x_i-f(y_i)||_1+2s_i
\tag{6}\]</span></span></p>
<p>To ensure stable performance in both high-level and low-lvel image processing applications, the authors proposed an adaptive loss function defined by:</p>
<p><span id="eq-UDL"><span class="math display">\[
\mathcal{L}_{UDL}=\frac{1}{N}\sum\limits_{i=1}^N\\hat{s_i}||x_i-f(y_i)||_1
\tag{7}\]</span></span></p>
<p>Where <span class="math inline">\(\hat{s_i}=s_i-\min{s_i}\)</span>; <span class="math inline">\(i=1,\ldots , N\)</span> is a non-negative linear scaling function.</p>
<p>To prevent uncertainty value from degenerating into zeros, the result of uncertainty estimation network in the first step will be passed to the second step as the attention signal (<span class="math inline">\(s =\ln(\theta)\)</span>). The two step flow diagram of the proposed UDL is shown in <a href="#fig-fig1" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-fig1" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-fig1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="UDL-block.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-fig1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Block diagram of UDL implementation
</figcaption>
</figure>
</div>
</section>
<section id="two-main-components-of-the-udl" class="level2">
<h2 class="anchored" data-anchor-id="two-main-components-of-the-udl">Two main components of the UDL</h2>
<ol type="1">
<li><p><strong>Estimating Sparsity Uncertainty (ESU):</strong> This component estimates the pixel-wise uncertainty (variance) of the SR image. They use a Convolutional Neural Network (CNN) to predict the log variance, and regularize it using Jeffrey’s prior to promote sparsity in the uncertainty map. The loss used for this step is LESU.</p></li>
<li><p><strong>Uncertainty-Driven Loss (LUDL):</strong> This is the adaptive weighted loss that guides the SISR network. It uses the uncertainty map estimated by ESU to assign larger weights to high-uncertainty pixels, effectively prioritizing them during training. The loss is computed using (<a href="#eq-UDL" class="quarto-xref">Equation&nbsp;7</a>).</p></li>
</ol>
<p>The method estimates a pixel-wise variance map (uncertainty) along with the SR image. It is assumed that the laplace distribution charateristics, can be captured with the variance map which is a latent variable. The authors use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) to evaluate the performance of their proposed method. The experimental results demonstrate that the proposed UDL consistently outperforms traditional loss functions (MSE, L1) and other state-of-the-art SISR methods (including those that model uncertainty, like GRAM) in terms of PSNR and SSIM on several benchmark datasets.</p>
</section>
<section id="key-contributions-of-this-work" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions-of-this-work">Key contributions of this work</h2>
<p>The main contribution of the loss function based approach are:</p>
<ol type="1">
<li><p>A Bayesian estimation framework for SISR that simultaneously estimates the SR image and its uncertainty.</p></li>
<li><p>A new uncertainty-driven loss (UDL) that prioritizes high-uncertainty pixels during training.</p></li>
<li><p>A demonstration that UDL achieves better performance than traditional loss functions and other state-of-the-art methods without increasing computational cost during testing.</p></li>
</ol>
</section>
<section id="limitations-of-the-study" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-the-study">Limitations of the Study</h2>
<ol type="1">
<li><p>The authors did not provide a thorough analysis of the computational cost during the training phase, focusing primarily on the testing phase.</p></li>
<li><p>The method relies on a two-step training process, which may be more complex to implement and tune than single-step training methods.</p></li>
<li><p>The performance improvements, while consistent, are relatively modest in some cases.</p></li>
<li><p>The choice of Jeffrey’s prior for regularizing the uncertainty map is somewhat heuristic and may not be optimal for all types of images.</p></li>
</ol>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<ol type="1">
<li><p>The authors suggested exploring a deep equilibrium model for SISR by iteratively alternating between estimating the uncertainty (variance) and the mean value.</p></li>
<li><p>Investigate alternative priors for regularizing the uncertainty map.</p></li>
<li><p>Explore different network architectures for estimating uncertainty.</p></li>
<li><p>Develop end-to-end trainable UDL methods that do not require a two-step training process.</p></li>
<li><p>Apply the UDL framework to other low-level vision tasks.</p></li>
<li><p>Explore perceptual metrics in the loss function.</p></li>
</ol>
</section>
<section id="review-summary" class="level2">
<h2 class="anchored" data-anchor-id="review-summary">Review Summary</h2>
<p>Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts. Traditional loss functions, such as Mean Squared Error (MSE) or L1 loss, treat all pixels equally, disregarding the varying importance of textures and edges. Existing SISR methods often fail to adequately address this, motivating the need for spatially adaptive approaches <span class="citation" data-cites="ning2021uncertainty">Ning et al. (<a href="#ref-ning2021uncertainty" role="doc-biblioref">2021</a>)</span>.</p>
<p>To overcome these limitations, authors proposed an uncertainty-driven loss (UDL) for SISR, prioritizing pixels with high uncertainty (e.g., textures and edges) during training. By casting SISR as a Bayesian estimation problem, their method simultaneously estimates the SR image (mean) and its uncertainty (variance). UDL incorporates an Estimating Sparsity Uncertainty (ESU) component regularized with Jeffrey’s prior, ensuring a more accurate uncertainty map. This map then guides the uncertainty-driven loss itself (LUDL), weighting high-uncertainty pixels more heavily.</p>
<p>Experimental results demonstrated that the proposed UDL outperforms traditional loss functions and other SISR methods, achieving better Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) scores. Visual comparisons confirmed improved reconstruction of textures and edges. While promising, the authors note limitations regarding computational complexity during training and potential dataset biases. This work highlights the benefits of modeling uncertainty in SISR and provides a pathway for future research into adaptive loss functions for low-level vision tasks.</p>
<blockquote class="blockquote">
<p>Ning, Q., Dong, W., Li, X., Wu, J., &amp; Shi, G. (2021). Uncertainty-Driven Loss for Single Image Super-Resolution. <em>35th Conference on Neural Information Processing Systems (NeurIPS 2021)</em></p>
</blockquote>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ning2021uncertainty" class="csl-entry" role="listitem">
Ning, Qian, Weisheng Dong, Xin Li, Jinjian Wu, and Guangming Shi. 2021. <span>“Uncertainty-Driven Loss for Single Image Super-Resolution.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 16398–409.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>